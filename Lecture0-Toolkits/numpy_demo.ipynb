{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c79251",
   "metadata": {},
   "source": [
    "## Linear Operations using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ffb6d",
   "metadata": {},
   "source": [
    "You will be dealing with _**a lot**_ of maths (linear algebra) later on in this subject. As such, `numpy` will be your best friend sooner or later.\n",
    "\n",
    "With that, in this demo, we will go through some linear algebra operations using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c0a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26053e",
   "metadata": {},
   "source": [
    "### Linear Systems with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b07cf",
   "metadata": {},
   "source": [
    "In this section, we start with a simple exercise of solving some linear systems using numpy. Consider the following system:\n",
    "\n",
    "$$\n",
    "-2x_2 + 3x_3 = 1 \\\\\n",
    "3x_1 + 6x_2 - 3x_3 = -2 \\\\\n",
    "6x_1 + 6x_2 + 3x_3 = 5\n",
    "$$\n",
    "\n",
    "Elementary Linear Algebra, p. 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5974b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 -2  3]\n",
      " [ 3  6 -3]\n",
      " [ 6  6  3]] [ 1 -2  5]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0, -2, 3],[3, 6, -3], [6, 6, 3]])\n",
    "b = np.array([1, -2, 5])\n",
    "\n",
    "print(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2c145",
   "metadata": {},
   "source": [
    "With numpy, you can actually solve this system using one function, `linalg.solve()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee51d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.40191980e+16 -1.80143985e+16 -1.20095990e+16]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488c1a5",
   "metadata": {},
   "source": [
    "On another note, you can also solve this sytem using inverse `inv()` function.\n",
    "\n",
    "$$\n",
    "\\textrm{A}x = \\textrm{b}\\\\\n",
    "x = \\textrm{A}^{-1}b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5ff0ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.40191980e+16 -1.80143985e+16 -1.20095990e+16]\n",
      "[ 2.40191980e+16 -1.80143985e+16 -1.20095990e+16]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.inv(A).dot(b))\n",
    "print(np.dot(np.linalg.inv(A), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511dc82",
   "metadata": {},
   "source": [
    "### Numpy Simulation of Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c5742",
   "metadata": {},
   "source": [
    "When we discuss transformers later on, one of the equations or terms you'll encounter is attention.\n",
    "\n",
    "$$\n",
    "Attention(Q,K,V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt(d_k)})V\n",
    "$$\n",
    "\n",
    "$Q$, $K$ and $V$ variables in the equation are the _query_, _key_ and _vector_ matrices, respectively. And put simply, attention is like a scoring function which tells your neural model where to \"pay attention\" to based on these matrix values.\n",
    "\n",
    "In this section, we try to simulate and visualize _attention_ by using numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf4a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c1a2f",
   "metadata": {},
   "source": [
    "Consider that each of the vectors below are simple embeddings to each word in a random 3-word sentence.\n",
    "\n",
    "We concatenate those vectors to form our sentence matrix, _words_, where each row correspond to a word in our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8004d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27392337 -0.46042657 -0.91805295 -0.96694473]\n",
      " [ 0.62654048  0.82551115  0.21327155  0.45899312]\n",
      " [ 0.08724998  0.87014485  0.63170711 -0.994523  ]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "word_1 = rng.uniform(-1, 1, (1,4))\n",
    "word_2 = rng.uniform(-1, 1, (1,4))\n",
    "word_3 = rng.uniform(-1, 1, (1,4))\n",
    "words = np.concatenate((word_1, word_2, word_3), axis=0)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48643e",
   "metadata": {},
   "source": [
    "Unlike RNNs which take each of these vectors sequentially, transformers take the entire matrix at once. While this can speed up computation, a problem, however, is they lose information order which is important whether you're processing texts, images, or audio.\n",
    "\n",
    "To address this, transformers require position embeddings. The idea is that these position embeddings will help the model track input positions, and overall give the sequence information. For our position encoder, we consider the following:\n",
    "\n",
    "$$\n",
    "\\textrm{PE}_{(\\textrm{pos},2i)} = \\sin\\Biggl(\\frac{\\textrm{pos}}{10000^{\\frac{2i}{d}}}\\Biggr)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textrm{PE}_{(\\textrm{pos},2i+1)} = \\cos\\Biggl(\\frac{\\textrm{pos}}{10000^{\\frac{2i}{d}}}\\Biggr)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    where $pos$ is the position of the corresponding word in the sentence, $i$ is the index of the word embedding, and $d$ the size of the embedding\n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44664efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 8.4147102e-01 9.0929741e-01 1.4112000e-01]\n",
      " [1.0000000e+00 9.9999768e-01 9.9999070e-01 9.9997914e-01]\n",
      " [0.0000000e+00 4.6415889e-06 9.2831779e-06 1.3924767e-05]]\n"
     ]
    }
   ],
   "source": [
    "def position_encoder(pos, d):\n",
    "    pos_matrix = np.indices((d,pos))[1].astype(np.float32)\n",
    "    i = 0\n",
    "    \n",
    "    for pos in pos_matrix:\n",
    "        if i%2 == 0:\n",
    "            pos_matrix[i,:] = np.sin(pos/(10000**(2*i/d)))\n",
    "        else:\n",
    "            pos_matrix[i,:] = np.cos(pos/(10000**(2*i/d)))\n",
    "        i+=1\n",
    "    return pos_matrix\n",
    "    \n",
    "d, pos = words.shape\n",
    "pos_matrix = position_encoder(pos,d)\n",
    "print(pos_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2e1cc",
   "metadata": {},
   "source": [
    "With our encoded words and position embeddings, our final input matrix simply becomes the sum of these two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b729f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27392337  0.38104444 -0.00875555 -0.82582473]\n",
      " [ 1.62654048  1.82550883  1.21326225  1.45897226]\n",
      " [ 0.08724998  0.87014949  0.63171639 -0.99450907]]\n"
     ]
    }
   ],
   "source": [
    "input = words + pos_matrix\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd26a7",
   "metadata": {},
   "source": [
    "Now, let us initialize our weights for our query, key, and value matrices from a Gaussian distribution. Our final $Q$, $K$, and $V$ matrices are simply obtained by multiplying these weights with our input.\n",
    "\n",
    "_**NOTE:**_ Normally, in training your neural models, the model will change these weights accordingly to what the model learned while training. However, in the case of this exercise, we will simply use these initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1581633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q=\n",
      " [[-0.0265223  -0.04311778 -0.05190939]\n",
      " [ 0.12195832  0.03121347 -0.01111102]\n",
      " [-0.07664195  0.12303517 -0.04965973]\n",
      " [-0.16251513  0.00187233 -0.09657318]]\n",
      "\n",
      "W_k=\n",
      " [[ 0.19754271 -0.03906768 -0.04669334]\n",
      " [ 0.07340005 -0.02955248 -0.07961355]\n",
      " [-0.09102963 -0.00310239 -0.10210198]\n",
      " [-0.18105258  0.1068872  -0.12617334]]\n",
      "\n",
      "W_v=\n",
      " [[-0.20949278 -0.04689471  0.09652361]\n",
      " [ 0.02770908  0.03028385 -0.16595225]\n",
      " [-0.11814195  0.08161342  0.16843546]\n",
      " [ 0.02038968  0.02171801  0.00386073]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu, sigma = 0, 0.1\n",
    "\n",
    "Wq = np.random.normal(mu, sigma, (pos,d))\n",
    "Wk = np.random.normal(mu, sigma, (pos,d))\n",
    "Wv = np.random.normal(mu, sigma, (pos,d))\n",
    "\n",
    "print(\"W_q=\\n {}\\n\".format(Wq))\n",
    "print(\"W_k=\\n {}\\n\".format(Wk))\n",
    "print(\"W_v=\\n {}\\n\".format(Wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fddfbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q=\n",
      " [[ 0.17408652 -0.00254071  0.06173432]\n",
      " [-0.15059546  0.13885326 -0.30586385]\n",
      " [ 0.2170147   0.09925964  0.05047469]]\n",
      "\n",
      "K=\n",
      " [[ 0.23239496 -0.11020529  0.06196432]\n",
      " [ 0.08071014  0.03468796 -0.52924372]\n",
      " [ 0.20365814 -0.13738385 -0.01236865]]\n",
      "\n",
      "V=\n",
      " [[-0.06263048 -0.0199559  -0.04145814]\n",
      " [-0.40375452  0.10971175  0.06404136]\n",
      " [-0.08907713  0.05221769 -0.03341767]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = np.matmul(input, Wq) # You can use np.matmul() or use @\n",
    "K = np.matmul(input, Wk)\n",
    "V = np.matmul(input, Wv)\n",
    "\n",
    "print(\"Q=\\n {}\\n\".format(Q))\n",
    "print(\"K=\\n {}\\n\".format(K))\n",
    "print(\"V=\\n {}\\n\".format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde6175",
   "metadata": {},
   "source": [
    "$d_k$ is the dimension of our queries and keys. We divide the dot product of $QK^T$ with $\\sqrt(d_k)$ as a normalization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032369e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7320508]\n"
     ]
    }
   ],
   "source": [
    "d_k = np.array([Q.shape[0]**0.5], dtype=np.float32)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18311f89",
   "metadata": {},
   "source": [
    "We get our attention score by taking the softmax of the dot product between our $Q$ and $K$ divided by $d_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e012f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33799243 0.32586828 0.3361393 ]\n",
      " [0.3173131  0.36107831 0.32160859]\n",
      " [0.33725009 0.3279609  0.33478901]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "atten_score = softmax(np.divide(Q.dot(K.transpose()), d_k), axis=1)\n",
    "print(atten_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e90b9",
   "metadata": {},
   "source": [
    "We finally calculate attention by getting the dot product of our softmax output to our value matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7a5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18268174  0.04655905 -0.00437648]\n",
      " [-0.19430844  0.05007592 -0.00077867]\n",
      " [-0.18335987  0.04673294 -0.00416657]]\n"
     ]
    }
   ],
   "source": [
    "atten = atten_score.dot(V)\n",
    "print(atten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c937d",
   "metadata": {},
   "source": [
    "It's easier to visualize this by plotting it in a heatmap.\n",
    "\n",
    "_**NOTE:**_ Install seaborn package using `pip install seaborn`\n",
    "\n",
    "The numbers `[0, 1, 2]` in our axes refers to each word in our sentence `[word_1, word_2, word_3]`, and you can consider the value on each cell as a score or weight of \"how much attention\" the model gives to other words for each word (including itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "593648ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "htmp = sns.heatmap(atten, annot=True, cmap=sns.light_palette(\"orange\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dce9a",
   "metadata": {},
   "source": [
    "### Notes on Other Common Numpy Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec58bb",
   "metadata": {},
   "source": [
    "Below are some numpy functions that you might want to familiarize with, as you will most likely use or encounter them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33799996",
   "metadata": {},
   "source": [
    "**1. `np.squeeze`, and `np.expand_dims`**\n",
    "\n",
    "Neural models usually require inputs (or output tensors) of shape `(N, C, H, W)`, where _N_ is batch size, _C_ is channels, _H_ is height of input, and _W_ is the width. Hence, to view a 4-dim tensor of an image (for example, with _N_=1) in 3-dim numpy would require the use of `np.squeeze`. `np.expand_dims` is its opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb98b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (293, 480, 3)\n",
      "Expanded image at axis=0: (1, 293, 480, 3)\n",
      "Squeezed image: (293, 480, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('data/car.jpg')\n",
    "image = np.array(image)\n",
    "\n",
    "print(\"Original image shape: {}\".format(image.shape))\n",
    "image = np.expand_dims(image, axis=0)\n",
    "print(\"Expanded image at axis=0: {}\".format(image.shape))\n",
    "image = image.squeeze()\n",
    "print(\"Squeezed image: {}\".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093516d",
   "metadata": {},
   "source": [
    "**2. `np.transpose` vs `np.reshape`**\n",
    "\n",
    "`np.transpose` permutes axes of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "575d2c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL Image: (293, 480, 3)\n",
      "Simulated tensor: (1, 3, 293, 480)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('data/car.jpg') # PIL: (H, W, C)\n",
    "image = np.array(image)\n",
    "print(\"PIL Image: {}\".format(image.shape))\n",
    "\n",
    "image = np.expand_dims(image.transpose(2,0,1), axis=0)\n",
    "print(\"Simulated tensor: {}\".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583c111",
   "metadata": {},
   "source": [
    "`np.reshape` reshapes the array to a potentially new dimension, while retaining the original data within the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d9cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL Image: (293, 480, 3)\n",
      "Reshaped image: (210960, 2)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('data/car.jpg') # PIL: (H, W, C)\n",
    "image = np.array(image)\n",
    "print(\"PIL Image: {}\".format(image.shape))\n",
    "\n",
    "h, w, c = image.shape\n",
    "image = image.reshape((int(h*w*c/2),2))\n",
    "print(\"Reshaped image: {}\".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189c043",
   "metadata": {},
   "source": [
    "**3. `np.concatenate`, `np.vstack`, and `np.hstack`**\n",
    "\n",
    "`np.concatenate` combines arrays along a specified axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629e407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[0 1 4]\n",
      " [2 3 5]]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "a2 = np.array([[4, 5]])\n",
    "\n",
    "print(np.concatenate((a1, a2), axis=0))\n",
    "print(np.concatenate((a1, a2.transpose()), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f6219",
   "metadata": {},
   "source": [
    "`np.vstack` combines (\"stacks\") array row-wise. It's similar to concatenating along `axis=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f617ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "a2 = np.array([[4, 5]])\n",
    "\n",
    "print(np.vstack((a1, a2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc968f",
   "metadata": {},
   "source": [
    "`np.hstack` combines (\"stacks\") array row-wise. It's similar to concatenating along `axis=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef3e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 4]\n",
      " [2 3 5]]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "a2 = np.array([[4, 5]])\n",
    "\n",
    "print(np.hstack((a1, a2.transpose())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e907406",
   "metadata": {},
   "source": [
    "**4. `np.flatten` vs `np.ravel`**\n",
    "\n",
    "`np.flatten` collapses the array into 1-dim. `np.ravel` is similar to flatten, except doesn't necessarily have to be a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9562c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "print(a1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4eac8e",
   "metadata": {},
   "source": [
    "**5. A note on `np.copy`**\n",
    "\n",
    "If you wish to copy initial values of an array to another, please consider using `np.copy` instead. Otherwise, any changes to your new array will also be reflected on the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fbc253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = \n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "a2 = \n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "After copying:\n",
      "\n",
      "a1 = \n",
      "[[0 1]\n",
      " [5 6]]\n",
      "\n",
      "a2 = \n",
      "[[0 1]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Without using np.copy()\n",
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "a2 = a1\n",
    "print(\"a1 = \\n{}\\n\\na2 = \\n{}\".format(a1,a2))\n",
    "\n",
    "a2[1,:] = np.array([5, 6])\n",
    "print(\"\\nAfter copying:\\n\")\n",
    "print(\"a1 = \\n{}\\n\\na2 = \\n{}\".format(a1,a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42f7e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = \n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "a2 = \n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "After copying:\n",
      "\n",
      "a1 = \n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "a2 = \n",
      "[[0 1]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Using np.copy()\n",
    "a1 = np.array([[0, 1],[2, 3]])\n",
    "a2 = a1.copy()\n",
    "print(\"a1 = \\n{}\\n\\na2 = \\n{}\".format(a1,a2))\n",
    "\n",
    "a2[1,:] = np.array([5, 6])\n",
    "print(\"\\nAfter copying:\\n\")\n",
    "print(\"a1 = \\n{}\\n\\na2 = \\n{}\".format(a1,a2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
